{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mana5aS/Visual-Odometry-using-RGB-D-Images/blob/main/Visual_Odometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Odometry using RGB-D Images."
      ],
      "metadata": {
        "id": "IgSsHl7vhj8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePogeOtk36b0"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "\n",
        "from scipy.spatial import kdtree\n",
        "import numpy as np\n",
        "import copy\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOyOyM9jtar-",
        "outputId": "10578b50-4f04-4175-881e-3c0477022825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python==4.5.5.62 in /usr/local/lib/python3.7/dist-packages (4.5.5.62)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.5.5.62) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "#opencv version to use SIFT \n",
        "!pip install opencv-contrib-python==4.5.5.62 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGB_lo8hxepJ",
        "outputId": "615cf7d8-c2ff-4266-d663-51431858ac72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Add path to the dataset in the drive\n",
        "path = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Processing "
      ],
      "metadata": {
        "id": "f8d8DqqZebPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K53fTrIMxiHd"
      },
      "outputs": [],
      "source": [
        "def dataset_processing():\n",
        "    \"\"\"\n",
        "    Function to perform association of RGB Images and Depth Images based on timestamp\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        depth_time - timestamps of depth images\n",
        "        depth_file - filenames of depth images\n",
        "        rgb_time   - timestamps of rgb images\n",
        "        rgb_file   - filenames of rgb images\n",
        "        gt_time    - timestamps of ground truth\n",
        "        t_gt       - ground truth translations\n",
        "        q_gt       - ground truth quaternions (orientation)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #Parsing Depth Images\n",
        "    file1 = open(path + \"depth.txt\",\"r+\") \n",
        "    file1.seek(75)\n",
        "\n",
        "    depth_time = []\n",
        "    depth_file = []\n",
        "    flag = False\n",
        "    for line in file1:\n",
        "        if flag:\n",
        "            depth_time.append(float(line.split()[0]))\n",
        "            depth_file.append(line.split()[1])\n",
        "        flag = True\n",
        "\n",
        "    depth_time = np.asarray(depth_time,dtype=np.float128)\n",
        "    depth_file = np.asarray(depth_file)\n",
        "\n",
        "\n",
        "    #Parsing RGB Images \n",
        "    file2 = open(path + \"rgb.txt\",\"r+\") \n",
        "    file2.seek(77)\n",
        "    rgb_time = []\n",
        "    rgb_file = []\n",
        "    flag = False\n",
        "    for line in file2:\n",
        "        if flag:\n",
        "            rgb_time.append(float(line.split()[0]))\n",
        "            rgb_file.append(line.split()[1])\n",
        "        flag = True\n",
        "\n",
        "\n",
        "    rgb_time = np.asarray(rgb_time,dtype=np.float128)\n",
        "    rgb_file = np.asarray(rgb_file)\n",
        "\n",
        "    #Parsing Ground Truth \n",
        "    file3 = open(path + \"groundtruth.txt\",\"r+\") \n",
        "    file3.seek(101)\n",
        "\n",
        "    gt_time = []\n",
        "    gt_tx = []\n",
        "    gt_ty = []\n",
        "    gt_tz = []\n",
        "    gt_quatx = []\n",
        "    gt_quaty = []\n",
        "    gt_quatz = []\n",
        "    gt_quatw = []\n",
        "\n",
        "    for line in file3:\n",
        "        data = line.split()\n",
        "        gt_time.append(float(data[0]))\n",
        "        gt_tx.append(float(data[1]))\n",
        "        gt_ty.append(float(data[2]))\n",
        "        gt_tz.append(float(data[3]))\n",
        "        gt_quatx.append(float(data[4]))\n",
        "        gt_quaty.append(float(data[5]))\n",
        "        gt_quatz.append(float(data[6]))\n",
        "        gt_quatw.append(float(data[7]))\n",
        "\n",
        "    gt_time = np.asarray(gt_time,dtype=np.float128)\n",
        "    gt_tx = np.asarray(gt_tx)\n",
        "    gt_ty = np.asarray(gt_ty)\n",
        "    gt_tz = np.asarray(gt_tz)\n",
        "    t_gt = np.stack((gt_tx, gt_ty, gt_tz))\n",
        "    gt_quatx = np.asarray(gt_quatx)\n",
        "    gt_quaty = np.asarray(gt_quaty)\n",
        "    gt_quatz = np.asarray(gt_quatz)\n",
        "    gt_quatw = np.asarray(gt_quatw)\n",
        "    q_gt = np.stack((gt_quatx, gt_quaty, gt_quatz, gt_quatw))\n",
        "\n",
        "\n",
        "    return depth_time, depth_file, rgb_time, rgb_file, gt_time, t_gt, q_gt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX7PwgCLxxOF"
      },
      "outputs": [],
      "source": [
        "\"\"\" Function to find matching RGB Images and Ground Truth values for each Depth Image - based on timestamp \"\"\"\n",
        "img_idx = lambda t: np.argmin(np.abs(rgb_time - t))\n",
        "img_idx2 = lambda t: np.argmin(np.abs(gt_time - t))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2D Feature Extraction and 3D Distribution Generation (with modeled depth uncertainty) "
      ],
      "metadata": {
        "id": "WIljxKDze3ue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYZff8wcCWDv"
      },
      "outputs": [],
      "source": [
        "def get_z_mean_var(u, v, depth, size=3):\n",
        "    \"\"\"\n",
        "    Function that uses Gaussian Mixture Models to model the uncertainty in depth (z) and\n",
        "    return mean and variance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        u, v  - image pixels\n",
        "        depth - (H,W) - depth image \n",
        "        size  - window size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        z_mean - mean of depth \n",
        "        sz_sq  - variance of depth\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    G_kernel = np.array([[1, 2 , 1],[2, 4, 2], [1, 2, 1]]) * (1/16)  # gaussian kernel\n",
        "    factor = 5000\n",
        "    \n",
        "    depth_padded = np.pad(depth, 1, mode='constant')\n",
        "    \n",
        "    # window  \n",
        "    rs = (v + 1) - size//2\n",
        "    re = (v + 1) + size//2 + 1\n",
        "    cs = (u + 1) - size//2\n",
        "    ce = (u + 1) + size//2 + 1\n",
        "\n",
        "    # filtered depth values\n",
        "    depth_window = depth_padded[rs:re, cs:ce] / factor\n",
        "\n",
        "    # z mean\n",
        "    weighted_depth = G_kernel * depth_window\n",
        "    z_mean = np.sum(weighted_depth)\n",
        "\n",
        "    # z variance\n",
        "    depth_sq_window = np.square(depth_window)\n",
        "    std_window = (0.001425) * depth_sq_window \n",
        "    var_window = np.square(std_window)\n",
        "    temp = G_kernel * (var_window + depth_sq_window) \n",
        "    sz_sq = np.sum(temp) - (z_mean**2)  \n",
        "\n",
        "    return z_mean, sz_sq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u8zeGKhCyql"
      },
      "outputs": [],
      "source": [
        "def cov_GMM(u, v, z, sz_sq, fx, fy, cx, cy, depth):\n",
        "    \"\"\"\n",
        "    Function that builds the covariance matrix for each feature using the \n",
        "    newly calculated z mean and variance (after noise modeling)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        u, v   - image pixels\n",
        "        z      - z mean\n",
        "        sz_sq  - z variance\n",
        "        fx, fy - focal lengths\n",
        "        cx, cy - optical centers\n",
        "        depth  - (H,W) - depth image \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        D_cov  - Covariance Matrix \n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    su = 1.0    # standard deviation of u pixel coordinate\n",
        "    sv = 1.0    # standard deviation of v pixel coordinate\n",
        "\n",
        "    sx_sq = (sz_sq*(u-cx)*(v-cy) + (su**2 * (z**2 + sz_sq))) / fx**2\n",
        "    sy_sq = (sz_sq*(u-cx)*(v-cy) + (sv**2 * (z**2 + sz_sq))) / fy**2\n",
        "    sxz = sz_sq*(u-cx) / fx\n",
        "    syz = sz_sq*(v-cy) / fy\n",
        "    sxy = (sz_sq*(u-cx)*(v-cy)) / (fx*fy)\n",
        "    szx = sxz\n",
        "    szy = syz\n",
        "    syx = sxy\n",
        "    D_cov = np.array([[sx_sq, sxy, sxz],\n",
        "                    [sxy, sy_sq, syz],\n",
        "                    [sxz, szy, sz_sq]])\n",
        "      \n",
        "    return D_cov\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7BkIxiFMHdu"
      },
      "outputs": [],
      "source": [
        "def get_data_gftt(image, depth):\n",
        "    \"\"\"\n",
        "    Function that extracts Shi-Tomasi Features and builds feature set D.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        image - (H,W) - RGB Image \n",
        "        depth - (H,W) - Depth Image \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        D_mean - (N,4) - Mean of each feature   \n",
        "        D_cov  - (N,3,3) - Covariance Matrix of each feature  \n",
        "        \n",
        "    \"\"\"  \n",
        "\n",
        "    #### Feature Extraction - Shi-Tomasi Features   \n",
        "    image = np.asarray(image)\n",
        "    depth = np.asarray(depth)\n",
        "    gray = cv.cvtColor(image , cv.COLOR_BGR2GRAY)\n",
        "    corners = cv.goodFeaturesToTrack(gray,400,0.01,2)\n",
        "    corners = np.int0(corners)\n",
        "\n",
        "    fx = 525.0  # focal length x\n",
        "    fy = 525.0  # focal length y\n",
        "    cx = 319.5  # optical center x\n",
        "    cy = 239.5  # optical center y\n",
        "\n",
        "    factor = 5000 # for the 16-bit PNG files\n",
        "\n",
        "    #### Calculate D_mean and D_cov\n",
        "    D_mean, D_cov = [], []\n",
        "\n",
        "    max_z_mean = 5.5\n",
        "    max_z_var = 0.03\n",
        "    for i in corners:\n",
        "        u, v = i.ravel()\n",
        "        z = depth[v, u] / factor    # raw depth value\n",
        "        if z == 0:                  # missing/invalid depth values (according to dataset)\n",
        "            continue\n",
        "        \n",
        "        # get mean and variance of raw depth value\n",
        "        z_mean, sz_sq = get_z_mean_var(u, v, depth)\n",
        "        if((z_mean > max_z_mean) or (sz_sq > max_z_var)):  # removing outliers\n",
        "            continue\n",
        "\n",
        "        z = z_mean\n",
        "        x = (u - cx) * z / fx\n",
        "        y = (v - cy) * z / fy\n",
        "\n",
        "        D_mean.append([x,y,z,1])    \n",
        "        D_cov.append(cov_GMM(u, v, z, sz_sq, fx, fy, cx, cy, depth))\n",
        "  \n",
        "\n",
        "    D_mean = np.asarray(D_mean)\n",
        "    D_cov = np.asarray(D_cov)\n",
        "\n",
        "    return D_mean, D_cov\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_sift(image, depth): \n",
        "    \"\"\"\n",
        "    Function that extracts SIFT Features and builds feature set D.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        image - (H,W) - RGB Image \n",
        "        depth - (H,W) - Depth Image \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        D_mean - (N,4) - Mean of each feature   \n",
        "        D_cov  - (N,3,3) - Covariance Matrix of each feature  \n",
        "        \n",
        "    \"\"\"   \n",
        "\n",
        "    #### Feature Extraction - SIFT Features\n",
        "    image = np.asarray(image)\n",
        "    depth = np.asarray(depth)\n",
        "    gray = cv.cvtColor(image , cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    sift = cv.xfeatures2d.SIFT_create(400)\n",
        "    kp = sift.detect(gray, None)\n",
        "    pts = cv.KeyPoint_convert(kp)\n",
        "    pts = np.round(pts).astype(int)\n",
        "\n",
        "    fx = 525.0  # focal length x\n",
        "    fy = 525.0  # focal length y\n",
        "    cx = 319.5  # optical center x\n",
        "    cy = 239.5  # optical center y\n",
        "\n",
        "    factor = 5000 # for the 16-bit PNG files\n",
        "\n",
        "    #### Calculate D_mean and D_cov\n",
        "    D_mean, D_cov = [], []\n",
        "\n",
        "    max_z_mean = 5.5\n",
        "    max_z_var = 0.03\n",
        "    for pt in pts:\n",
        "        u = pt[0]\n",
        "        v = pt[1]\n",
        "        z = depth[v, u] / factor  \n",
        "        if z == 0:\n",
        "            continue\n",
        "        \n",
        "        z_mean, sz_sq = get_z_mean_var(u, v, depth)\n",
        "        if((z_mean > max_z_mean) or (sz_sq > max_z_var)):\n",
        "            continue\n",
        "\n",
        "        z = z_mean\n",
        "        x = (u - cx) * z / fx\n",
        "        y = (v - cy) * z / fy\n",
        "\n",
        "        D_mean.append([x,y,z,1])    \n",
        "        D_cov.append(cov_GMM(u, v, z, sz_sq, fx, fy, cx, cy, depth))\n",
        "\n",
        "    D_mean = np.asarray(D_mean)\n",
        "    D_cov = np.asarray(D_cov)\n",
        "\n",
        "    return D_mean, D_cov"
      ],
      "metadata": {
        "id": "mKR5PTqjvOoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_orb(image, depth): \n",
        "    \"\"\"\n",
        "    Function that extracts ORB Features and builds feature set D.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        image - (H,W) - RGB Image \n",
        "        depth - (H,W) - Depth Image \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        D_mean - (N,4) - Mean of each feature   \n",
        "        D_cov  - (N,3,3) - Covariance Matrix of each feature  \n",
        "        \n",
        "    \"\"\"  \n",
        "\n",
        "    #### Feature Extraction - ORB Features  \n",
        "    image = np.asarray(image)\n",
        "    depth = np.asarray(depth)\n",
        "    gray = cv.cvtColor(image , cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    orb = cv.ORB_create(400)\n",
        "    kp = orb.detect(gray, None)\n",
        "    pts = cv.KeyPoint_convert(kp)\n",
        "    pts = np.round(pts).astype(int)\n",
        "\n",
        "    fx = 525.0  # focal length x\n",
        "    fy = 525.0  # focal length y\n",
        "    cx = 319.5  # optical center x\n",
        "    cy = 239.5  # optical center y\n",
        "\n",
        "    factor = 5000 # for the 16-bit PNG files\n",
        "\n",
        "    #### Calculate D_mean and D_cov\n",
        "    D_mean, D_cov = [], []\n",
        "\n",
        "    max_z_mean = 5.5\n",
        "    max_z_var = 0.03\n",
        "    for pt in pts:\n",
        "        u = pt[0]\n",
        "        v = pt[1]\n",
        "        z = depth[v, u] / factor   \n",
        "        if z == 0:\n",
        "            continue\n",
        "        \n",
        "        z_mean, sz_sq = get_z_mean_var(u, v, depth)\n",
        "        if((z_mean > max_z_mean) or (sz_sq > max_z_var)):\n",
        "            continue\n",
        "\n",
        "        z = z_mean\n",
        "        x = (u - cx) * z / fx\n",
        "        y = (v - cy) * z / fy\n",
        "\n",
        "        D_mean.append([x,y,z,1])    \n",
        "        D_cov.append(cov_GMM(u, v, z, sz_sq, fx, fy, cx, cy, depth))\n",
        "\n",
        "    D_mean = np.asarray(D_mean)\n",
        "    D_cov = np.asarray(D_cov)\n",
        "\n",
        "    return D_mean, D_cov"
      ],
      "metadata": {
        "id": "XXKnpE3VveAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Registration using Iterative Closest Point "
      ],
      "metadata": {
        "id": "1EH0PQ6AfXjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_EzKqxX4GtB"
      },
      "outputs": [],
      "source": [
        "def get_k_nearest_neigh(M_mean, D_mean, k):\n",
        "    \"\"\"\n",
        "    Function that uses KD-Tree to find the k nearest neighbors in M for every \n",
        "    feature in D\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        M_mean - (N,4) - mean of model (M) features\n",
        "        D_mean - (N,4) - mean of data (D) features\n",
        "        k              - no. of neighbors\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        dists - (N,k)  - Euclidean distances between each D features and its \n",
        "                         'k' neighbors\n",
        "        neigh_idxs - (N,) - indices of the neighbors in M\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tree = kdtree.KDTree(M_mean)\n",
        "    dists, neigh_idxs = tree.query(D_mean, k) \n",
        "    \n",
        "    return dists, neigh_idxs \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-c6Lgk__CI8"
      },
      "outputs": [],
      "source": [
        "def find_nearest_neigh_icp(M_mean, M_cov, D_mean, D_cov, euc_dist_th = 0.15):\n",
        "    \"\"\"\n",
        "    Function that finds the nearest neigbors for Registration (using ICP)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        M_mean  - (N,4)    - mean of model (M) features\n",
        "        M_cov   - (N,3,3)  - covariance of model (M) features\n",
        "        D_mean  - (N,4)    - mean of data (D') features\n",
        "        D_cov   - (N,3,3)  - covariance of data (D') features\n",
        "        euc_dist_th        - threshold for registration\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "        d_associated_idxs - indices in D that are associated for registration\n",
        "        m_associated_idxs - indices in M that are associated for registration\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    dists, neigh_idxs = get_k_nearest_neigh(M_mean, D_mean, k=1)\n",
        "\n",
        "    min_dist = np.inf\n",
        "    min_dist_idx = 0\n",
        "    d_associated_idxs = []\n",
        "    m_associated_idxs = []\n",
        "    nearest_dist = []\n",
        "\n",
        "    for i in range(D_mean.shape[0]):    \n",
        "        if dists[i] < euc_dist_th:\n",
        "            d_associated_idxs.append(i)\n",
        "            m_associated_idxs.append(neigh_idxs[i])\n",
        "            nearest_dist.append(dists[i])\n",
        "            \n",
        "    return d_associated_idxs, m_associated_idxs\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiBxuc5VmadQ"
      },
      "outputs": [],
      "source": [
        "def find_T(X, Y):\n",
        "    \"\"\"\n",
        "    Function that computes the transformation between two point clouds (Procrustes)\n",
        "    Y = RX + t\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        X, Y - (N,3), (N,3) - Two point clouds\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        T - (4,4) - Transformation matrix\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    X_centroid = np.mean(X.T,axis = 1).reshape(3,1)\n",
        "    Y_centroid = np.mean(Y.T,axis = 1).reshape(3,1)\n",
        "    A = X.T - X_centroid \n",
        "    B = Y.T - Y_centroid \n",
        "\n",
        "    u,s,vT = np.linalg.svd(A @ B.T)\n",
        "\n",
        "    S = np.eye(3)\n",
        "    S[2][2] = np.linalg.det(vT.T@u.T)\n",
        "    R = vT.T @ S @ u.T\n",
        "    t = (Y_centroid - (R @ X_centroid)).reshape(-1)\n",
        "\n",
        "    T = np.eye(4)\n",
        "    T[:3,:3] = copy.deepcopy(R)\n",
        "    T[:3,3] = copy.deepcopy(t)\n",
        "    \n",
        "    return T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oNTXJMGuHYi"
      },
      "outputs": [],
      "source": [
        "def icp(D_mean, D_cov, M_mean, M_cov, T_prev, max_iter=10, linear_tol=1e-4, angular_tol=1.7e-3, min_corres = 15):\n",
        "    \"\"\"\n",
        "    Function that performs registration: finds the best estimate of the transformation matrix\n",
        "    between Data feature set (D) and Model feature set (M)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        D_mean - (N,4)   - mean of data (D) features\n",
        "        D_cov  - (N,3,3) - covariance of data (D) features\n",
        "        M_mean - (N,4)   - mean of model (M) features\n",
        "        M_cov  - (N,3,3) - covariance of model (M) features\n",
        "        T_prev - (4,4)   - previous estimate of T\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "        T_final - (4,4) - best estimate of transformation matrix\n",
        "\n",
        "    \"\"\"    \n",
        "\n",
        "    # Transform dataset D_mean to fixed frame using preious transformation estimate\n",
        "    D_mean_transformed = (T_prev @ D_mean.T).T\n",
        "    R = T_prev[:3,:3]\n",
        "    D_cov_transformed = R @ D_cov @ R.T\n",
        "\n",
        "    T_final = np.eye(4)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        d_assoc_idxs, m_assoc_idxs = find_nearest_neigh_icp(M_mean, M_cov, D_mean_transformed, D_cov_transformed)\n",
        "\n",
        "        if len(m_assoc_idxs) < min_corres:\n",
        "            break\n",
        "\n",
        "        # find new transformation matrix\n",
        "        T_corr = find_T(D_mean_transformed[d_assoc_idxs, :3], M_mean[m_assoc_idxs,:3])\n",
        "\n",
        "        # update D_mean\n",
        "        D_mean_transformed = (T_corr @ D_mean_transformed.T).T\n",
        "        # accumulate Final transformation matrix\n",
        "        T_final = T_corr @ T_final\n",
        "        \n",
        "        # check for convergence\n",
        "        t = T_corr[:3,3]\n",
        "        R = T_corr[:3,:3]\n",
        "\n",
        "        # linear error\n",
        "        linear_err = np.linalg.norm(t)\n",
        "        tr = R[0,0] + R[1,1] + R[2,2]\n",
        "\n",
        "        # angular_err \n",
        "        cos_theta = (tr - 1.0)/2.0\n",
        "        cos_theta = np.clip(cos_theta,-1,1)\n",
        "        angular_err = np.arccos(cos_theta)\n",
        "        if((linear_err <linear_tol) and (angular_err < angular_tol)):\n",
        "            break\n",
        "\n",
        "    return T_final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Association "
      ],
      "metadata": {
        "id": "vJA1Rj4Afg9j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVUjrN1pAlBW"
      },
      "outputs": [],
      "source": [
        "def mahalanobis_dist(d_mean, d_cov, m_mean, m_cov):\n",
        "    \"\"\"\n",
        "    Function that calculates the Mahalanobis distance between Data feature set D\n",
        "    and Model feature set M\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_mean - (N,4) - mean of data (D) features \n",
        "        d_cov  - (N,3,3) - covariance of data (D) features\n",
        "        m_mean - (N,4) - mean of model (M) features\n",
        "        m_cov  - (N,3,3) - covariance of model (M) features\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        d - (1,N) - Mahalanobis distances \n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    delta = m_mean - d_mean\n",
        "    delta = delta[:,:3]\n",
        "    d_2 = delta[:,np.newaxis,:] @ np.linalg.inv(m_cov + d_cov) @ delta[:,:,np.newaxis]\n",
        "    d = np.sqrt(d_2)\n",
        "\n",
        "    return d.reshape(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btukpukoGu5F"
      },
      "outputs": [],
      "source": [
        "def find_nearest_neigh_assoc(M_mean, M_cov, D_mean, D_cov, maha_dist_th):\n",
        "    \"\"\"\n",
        "    Function that finds the nearest neigbors for Data Association\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        M_mean - (N,4)   - mean of model (M) features\n",
        "        M_cov  - (N,3,3) - covariance of model (M) features\n",
        "        D_mean - (N,4)   - mean of data (D') features\n",
        "        D_cov  - (N,3,3) - covariance of data (D') features\n",
        "        maha_dist_th     - threshold for association\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "        d_associated_idxs - indices in Dp that are associated\n",
        "        m_associated_idxs - indices in M that are associated\n",
        "        nearest_dist      - nearest distances\n",
        "\n",
        "    \"\"\"    \n",
        "\n",
        "    d_associated_idxs = []\n",
        "    m_associated_idxs = []\n",
        "    nearest_dist = []\n",
        "\n",
        "    neigh_idxs = get_k_nearest_neigh(M_mean, D_mean, k=4)[1]\n",
        "\n",
        "    for i in range(D_mean.shape[0]):\n",
        "        dist = mahalanobis_dist(D_mean[i], D_cov[i], M_mean[neigh_idxs[i]], M_cov[neigh_idxs[i]])\n",
        "        if np.isnan(dist).all():\n",
        "            continue        \n",
        "        else:\n",
        "            min_dist = np.nanmin(dist)\n",
        "            nearest_dist.append(min_dist)\n",
        "            if min_dist < maha_dist_th:\n",
        "                d_associated_idxs.append(i)\n",
        "                m_associated_idxs.append(neigh_idxs[i][np.nanargmin(dist)])\n",
        "            \n",
        "    return d_associated_idxs, m_associated_idxs, nearest_dist\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62J4zCDMMRr-"
      },
      "outputs": [],
      "source": [
        "def association_idx(Dp_mean, Dp_cov, M_mean, M_cov, th = 11.35):\n",
        "    \"\"\"\n",
        "    Function that returns the indices of associated features in Dp and M\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        Dp_mean - (N,4)     - mean of data (D') features\n",
        "        Dp_cov  - (N,3,3)   - covariance of data (D') features\n",
        "        M_mean  - (N,4)     - mean of model (M) features\n",
        "        M_cov   - (N,3,3)   - covariance of model (M) features\n",
        "        th                  - threshold for association\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "        d_assoc_idxs - indices in Dp that are associated\n",
        "        m_assoc_idxs - indices in M that are associated\n",
        "        dist_arr     - all distances (for plotting)\n",
        "\n",
        "    \"\"\"   \n",
        "     \n",
        "    # obtaining associated indices\n",
        "    d_assoc_idxs, m_assoc_idxs, dist_arr = find_nearest_neigh_assoc(M_mean, M_cov, Dp_mean, Dp_cov, th)\n",
        "\n",
        "    dist_arr = np.asarray(dist_arr)\n",
        "    dist_arr = np.squeeze(dist_arr)\n",
        "    dist_arr = dist_arr.flatten()\n",
        "\n",
        "\n",
        "    return d_assoc_idxs, m_assoc_idxs, dist_arr          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPoJCGtYMbou"
      },
      "outputs": [],
      "source": [
        "def Kalman_Update(D_assoc_mean, D_assoc_cov, M_assoc_mean, M_assoc_cov): \n",
        "    \"\"\"\n",
        "    Function that updates associated features in Model M using Kalman Filter\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        D_assoc_mean  - (N,4)       - mean of associated data (D') features\n",
        "        D_assoc_cov   - (N,3,3)     - covariance of associated data (D') features\n",
        "        M_assoc_mean  - (N,4)       - mean of associated model (M) features\n",
        "        M_assoc_cov   - (N,3,3)     - covariance of associated model (M) features\n",
        "       \n",
        "    Returns\n",
        "    -------\n",
        "        M_upd_mean  - (N,4)     - Updated Mean value of Model (M)\n",
        "        M_upd_cov   - (N,3,3)   - Updated Covariance value of Model (M)\n",
        "\n",
        "    \"\"\"   \n",
        "\n",
        "    #propagate dynamics\n",
        "    inter_mean = copy.deepcopy(M_assoc_mean)   #(N,4)\n",
        "    inter_cov = copy.deepcopy(M_assoc_cov)  #(N,3,3)\n",
        "\n",
        "    #updating using observations\n",
        "    K = inter_cov @ np.linalg.inv(inter_cov + D_assoc_cov)\n",
        "    \n",
        "    M_upd_mean = (inter_mean[:, :3, np.newaxis] + (K @ np.subtract(D_assoc_mean, inter_mean)[:, :3, np.newaxis])).reshape(-1,3)\n",
        "    I = np.eye(3)\n",
        "    M_upd_cov = np.subtract(I, K) @ inter_cov\n",
        "    \n",
        "    M_upd_mean = np.hstack((np.array(M_upd_mean), np.ones((len(M_upd_mean), 1))))\n",
        "    M_upd_cov = np.array(M_upd_cov)\n",
        "\n",
        "\n",
        "    return M_upd_mean, M_upd_cov    #(N,4), (N,3,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "BzVIULduh3nT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9jNlyci1Sux"
      },
      "outputs": [],
      "source": [
        "\"\"\" Main Function \"\"\"\n",
        "\n",
        "#Select feature detector\n",
        "feature_detector = \"GFTT\"    # \"GFTT\" or \"SIFT\" or \"ORB\"\n",
        "\n",
        "# dataset processing \n",
        "depth_time, depth_file, rgb_time, rgb_file, gt_time, t_gt, q_gt = dataset_processing()\n",
        "\n",
        "# initialization\n",
        "T_est = np.zeros((len(depth_time), 4, 4))\n",
        "T_prev = np.array([[0.9637 , -0.1439 , 0.2246 , -0.0730],      #initial seed\n",
        "                   [-0.2666 , -0.5479 , 0.7928 , -0.4169],\n",
        "                   [0.00901 , -0.8240 , -0.5664 , 1.5916],\n",
        "                   [0.0 , 0.0 , 0.0 , 1.0]])\n",
        "T_est[0,:,:] = copy.deepcopy(T_prev)\n",
        "M_mean_size = np.zeros(len(depth_time))\n",
        "\n",
        "# max model size\n",
        "model_size = 5000\n",
        "\n",
        "# storing corresponding ground truth values\n",
        "t_gt_new = np.zeros((len(depth_time), 3))\n",
        "q_gt_new = np.zeros((len(depth_time), 4))\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in tqdm(range(len(depth_time))):\n",
        "\n",
        "    # getting matching rgb and ground truth indices\n",
        "    idx = img_idx(depth_time[i])\n",
        "    idx2 = img_idx2(depth_time[i])\n",
        "\n",
        "    # reading images\n",
        "    depth_I = Image.open((path + depth_file[i]) )\n",
        "    rgb_I = Image.open(path + rgb_file[idx])\n",
        "\n",
        "    # storing matching ground truth values\n",
        "    t_gt_new[i, :] = t_gt[:, idx2]\n",
        "    q_gt_new[i, :] = q_gt[:, idx2]\n",
        "\n",
        "    # building feature set\n",
        "    if feature_detector == \"GFTT\":\n",
        "        D_mean, D_cov = get_data_gftt(rgb_I, depth_I)\n",
        "    elif  feature_detector == \"SIFT\":\n",
        "        D_mean, D_cov = get_data_sift(rgb_I, depth_I)\n",
        "    elif  feature_detector == \"ORB\":\n",
        "        D_mean, D_cov = get_data_orb(rgb_I, depth_I)\n",
        "\n",
        "    # Initializing Model\n",
        "    if(i == 0):\n",
        "        M_mean = (T_prev @ D_mean.T).T\n",
        "        M_cov = T_prev[:3, :3] @ D_cov @ T_prev[:3, :3].T\n",
        "        M_mean_size[i] = len(M_mean)\n",
        "        continue\n",
        "\n",
        "    # registration between data and model using ICP\n",
        "    # estimating transformation matrix\n",
        "    correction = icp(D_mean, D_cov, M_mean, M_cov, T_prev)\n",
        "    T_curr = correction @ T_prev\n",
        "    T_est[i, :, :] = copy.deepcopy(T_curr)\n",
        "    T_prev = copy.deepcopy(T_curr)\n",
        "\n",
        "    # creating dataset D_prime in fixed frame\n",
        "    Dp_mean = (T_curr @ D_mean.T).T\n",
        "    Dp_cov =  T_curr[:3, :3] @ D_cov @ T_curr[:3, :3].T\n",
        "\n",
        "    # Data Association \n",
        "    idx_D, idx_M, dist_arr = association_idx(Dp_mean, Dp_cov, M_mean, M_cov, 10.0)\n",
        "\n",
        "    # Update Associated Features\n",
        "    if len(idx_D) != 0:\n",
        "        D_assoc_mean = Dp_mean[idx_D, :]\n",
        "        D_assoc_cov = Dp_cov[idx_D, :, :]\n",
        "        M_assoc_mean = M_mean[idx_M, :]\n",
        "        M_assoc_cov = M_cov[idx_M, :, :]\n",
        "\n",
        "        # Kalman Filter Update\n",
        "        M_upd_mean, M_upd_cov = Kalman_Update(D_assoc_mean, D_assoc_cov, M_assoc_mean, M_assoc_cov)\n",
        "        M_mean[idx_M, :] = M_upd_mean\n",
        "        M_cov[idx_M, :, :] = M_upd_cov\n",
        "\n",
        "\n",
        "    if(len(idx_D) < D_mean.shape[0]):\n",
        "        if len(idx_D) == 0:\n",
        "            # all new features\n",
        "            new_feature_idx = np.arange(D_mean.shape[0])    \n",
        "        else:\n",
        "            # add non-associated features to model M as new features\n",
        "            new_feature_idx = np.array(list(set(np.arange(D_mean.shape[0])) - set(idx_D)))\n",
        "\n",
        "        M_mean = np.append(M_mean, Dp_mean[new_feature_idx, :], axis=0)\n",
        "        M_cov = np.append(M_cov, Dp_cov[new_feature_idx, :, :], axis=0)\n",
        "     \n",
        "    # pruning model\n",
        "    if(M_mean.shape[0] > model_size):\n",
        "        diff = M_mean.shape[0] - model_size\n",
        "        M_mean = np.delete(M_mean, np.s_[:diff], 0)\n",
        "        M_cov = np.delete(M_cov, np.s_[:diff], 0)\n",
        "\n",
        "    M_mean_size[i] = len(M_mean)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Start Time = \", start_time)\n",
        "print(\"End Time = \", end_time)\n",
        "print(\"Time Taken (hr) = \", (end_time - start_time)/3600)\n",
        "\n",
        "save_time = str(time.time())\n",
        "print(\"Save Time = \", save_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Results "
      ],
      "metadata": {
        "id": "VLK4O5Idg5U3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9isqtm0x40pq"
      },
      "outputs": [],
      "source": [
        "\"\"\" Saving Data \"\"\"\n",
        "save_path = path + 'Results/'\n",
        "np.save(save_path + 'T_est_' + save_time + '.npy', T_est)\n",
        "np.save(save_path + 't_gt_' + save_time + '.npy', t_gt_new)\n",
        "np.save(save_path + 'q_gt_' + save_time + '.npy', q_gt_new)\n",
        "np.save(save_path + 'timestamps_' + save_time + '.npy', np.arange(len(depth_time)))\n",
        "np.save(save_path + 'M_mean_' + save_time + '.npy', M_mean)\n",
        "np.save(save_path + 'M_mean_size_' + save_time + '.npy', M_mean_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6hdNi3GVI1H"
      },
      "outputs": [],
      "source": [
        "def quat_to_euler(q):\n",
        "    \"\"\"\n",
        "    Function that converts quaternions to Euler Angles\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        q - quaternion [x,y,z,w]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        phi, theta, psi - roll, pitch, yaw angles\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    phi = math.atan2(2*(q[3]*q[0]+q[1]*q[2]), \\\n",
        "            1 - 2*(q[0]**2 + q[1]**2))\n",
        "    theta = math.asin(2*(q[3]*q[1] - q[2]*q[0]))\n",
        "    psi = math.atan2(2*(q[3]*q[2]+q[0]*q[1]), \\\n",
        "            1 - 2*(q[1]**2 + q[2]**2))\n",
        "    \n",
        "    return np.array([phi, theta, psi])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6OtW2LXWdvd"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.transform import Rotation as R\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "\n",
        "def plot_results(T_est, t_gt, q_gt, time_steps, M_mean, M_mean_size, full_model,  path, save_time):\n",
        "    \"\"\" Function responsible for plotting \"\"\"\n",
        "\n",
        "    print(\"Plotting...\")\n",
        "\n",
        "    r_est, p_est, y_est = [], [], []\n",
        "    r_gt, p_gt, y_gt = [], [], []\n",
        "    tx_est, ty_est, tz_est = [], [], []\n",
        "    tx_gt, ty_gt, tz_gt = [], [], []\n",
        "\n",
        "    \n",
        "    for i in range(len(time_steps)):\n",
        "        #### GET Rotation data ####\n",
        "        r, p, y = quat_to_euler(q_gt[i])\n",
        "        r_gt.append(r)\n",
        "        p_gt.append(p)\n",
        "        y_gt.append(y)\n",
        "        q = R.from_matrix(T_est[i][:3, :3]).as_quat()\n",
        "        r, p, y = quat_to_euler(q)\n",
        "        r_est.append(r)\n",
        "        p_est.append(p)\n",
        "        y_est.append(y)\n",
        "\n",
        "        #### GET Translation data ####\n",
        "        tx, ty, tz = t_gt[i][0], t_gt[i][1], t_gt[i][2]\n",
        "        tx_gt.append(tx)\n",
        "        ty_gt.append(ty)\n",
        "        tz_gt.append(tz)\n",
        "        tx, ty, tz = T_est[i][0,3], T_est[i][1,3], T_est[i][2,3]\n",
        "        tx_est.append(tx)\n",
        "        ty_est.append(ty)\n",
        "        tz_est.append(tz)\n",
        "\n",
        "    save_path = path + 'Plots/'\n",
        "    \n",
        "    #Plot Rotations\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(time_steps, r_gt, 'r--', markersize=1.5, label='Ground Truth')\n",
        "    ax1.plot(time_steps, r_est, 'r-', markersize=2, label='Estimate')\n",
        "    ax1.set_xlabel('Time (secs)')\n",
        "    ax1.set_ylabel('Roll (rad)')\n",
        "    ax1.set_title('Roll - Ground truth vs Estimated')\n",
        "    ax1.legend()\n",
        "    plt.savefig(save_path + 'Roll/' + save_time + '.png')\n",
        "    # ax1.legend()\n",
        "\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(time_steps, p_gt, 'g--', markersize=1.5, label='Ground Truth')\n",
        "    ax2.plot(time_steps, p_est, 'g-', markersize=2, label='Estimate')\n",
        "    ax2.set_xlabel('Time (secs)')\n",
        "    ax2.set_ylabel('Pitch (rad)')\n",
        "    ax2.set_title('Pitch - Ground truth vs Estimated')\n",
        "    ax2.legend()\n",
        "    plt.savefig(save_path + 'Pitch/' + save_time + '.png')\n",
        "    # ax2.legend()\n",
        "\n",
        "    fig3, ax3 = plt.subplots()\n",
        "    ax3.plot(time_steps, y_gt, 'b--', markersize=1.5, label='Ground Truth')\n",
        "    ax3.plot(time_steps, y_est, 'b-', markersize=2, label='Estimate')\n",
        "    ax3.set_xlabel('Time (secs)')\n",
        "    ax3.set_ylabel('Yaw (rad)')\n",
        "    ax3.set_title('Yaw - Ground truth vs Estimated')\n",
        "    ax3.legend()\n",
        "    plt.savefig(save_path + 'Yaw/' + save_time + '.png')\n",
        "    # ax3.legend()\n",
        "\n",
        "    #Plot Translation\n",
        "    fig4, ax4 = plt.subplots()\n",
        "    ax4.plot(time_steps, tx_gt, 'r--', markersize=1.5, label='Ground Truth')\n",
        "    ax4.plot(time_steps, tx_est, 'r-', markersize=2, label='Estimate')\n",
        "    ax4.set_xlabel('Time (secs)')\n",
        "    ax4.set_ylabel('Translation along x-axis (m)')\n",
        "    ax4.set_title('Translation along x-axis - Ground truth vs Estimated')\n",
        "    ax4.legend()\n",
        "    plt.savefig(save_path + 'Tx/' + save_time + '.png')\n",
        "    # ax4.legend()\n",
        "\n",
        "    fig5, ax5 = plt.subplots()\n",
        "    ax5.plot(time_steps, ty_gt, 'g--', markersize=1.5, label='Ground Truth')\n",
        "    ax5.plot(time_steps, ty_est, 'g-', markersize=2, label='Estimate')\n",
        "    ax5.set_xlabel('Time (secs)')\n",
        "    ax5.set_ylabel('Translation along y-axis (m)')\n",
        "    ax5.legend()\n",
        "    ax5.set_title('Translation along y-axis - Ground truth vs Estimated')\n",
        "    plt.savefig(save_path + 'Ty/' + save_time + '.png')\n",
        "    # ax5.legend()\n",
        "\n",
        "    fig6, ax6 = plt.subplots()\n",
        "    ax6.plot(time_steps, tz_gt, 'b--', markersize=1.5, label='Ground Truth')\n",
        "    ax6.plot(time_steps, tz_est, 'b-', markersize=2, label='Estimate')\n",
        "    ax6.set_xlabel('Time (secs)')\n",
        "    ax6.set_ylabel('Translation along z-axis (m)')\n",
        "    ax6.legend()\n",
        "    ax6.set_title('Translation along z-axis - Ground truth vs Estimated')\n",
        "    plt.savefig(save_path + 'Tz/' + save_time + '.png')\n",
        "    # ax6.legend()\n",
        "\n",
        "    fig7, ax7 = plt.subplots()\n",
        "    ax7.plot(time_steps, M_mean_size, 'b--', markersize=1.5, label='Model Size')\n",
        "    ax7.set_xlabel('Time (secs)')\n",
        "    ax7.set_ylabel('Model size (num of features)')\n",
        "    ax7.legend()\n",
        "    ax7.set_title('Model Size')\n",
        "    plt.savefig(save_path + 'M_mean_size/' + save_time + '.png')\n",
        "\n",
        "    #Plot Model\n",
        "    fig = plt.figure(8)\n",
        "    axs = fig.gca(projection='3d')\n",
        "    axs.plot3D(tx_est, ty_est, tz_est, 'k-', markersize=1)\n",
        "    axs.plot3D([tx_est[0]], [ty_est[0]], [tz_est[0]], 'ro', markersize=5)\n",
        "    axs.plot3D([tx_est[-1]], [ty_est[-1]], [tz_est[-1]], 'go', markersize=5)\n",
        "    axs.set_xlabel('x-axis (m)')\n",
        "    axs.set_ylabel('y-axis (m)')\n",
        "    axs.set_zlabel('z-axis (m)')\n",
        "    axs.legend()\n",
        "    axs.set_title('Final Model Set')\n",
        "    plt.savefig(save_path + 'Model/' + save_time + '.png')\n",
        "\n",
        "    plt.show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjMLYs5A1veX"
      },
      "outputs": [],
      "source": [
        "\"\"\" Plotting Loaded Data \"\"\"\n",
        "\n",
        "save_path = path + 'Results/'\n",
        "#save_time = ''\n",
        "T_est = np.load(save_path + 'T_est_' + save_time + '.npy')\n",
        "t_gt = np.load(save_path + 't_gt_' + save_time + '.npy')\n",
        "q_gt = np.load(save_path + 'q_gt_' + save_time + '.npy')\n",
        "depth_time = np.load(save_path + 'timestamps_' + save_time + '.npy')\n",
        "M_mean = np.load(save_path + 'M_mean_' + save_time + '.npy')\n",
        "M_mean_size = np.load(save_path + 'M_mean_size_' + save_time + '.npy')\n",
        "full_model = np.load(save_path + 'Full_model_' + save_time + '.npy')\n",
        "\n",
        "plot_results(T_est, t_gt, q_gt, depth_time, M_mean, M_mean_size, full_model, path, save_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation "
      ],
      "metadata": {
        "id": "xcGD1XV0g9bB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBhRAp_p5WbL"
      },
      "outputs": [],
      "source": [
        "# Evaluation - Absolute Trajectory Error (ATE)\n",
        "def absolute_traj_error(t_gt, T_est):\n",
        "    \"\"\" \n",
        "    Function that evaluates the estimted trajectory with ground truth\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        t_gt  - ground truth trajectory\n",
        "        T_est - estimated trajectory\n",
        "    \"\"\"\n",
        "\n",
        "    t_est = T_est[:,:3,3]\n",
        "    t_err = np.linalg.norm((t_gt - t_est), axis=1) \n",
        "    t_err_rmse = np.sqrt(np.mean(t_err**2))\n",
        "    t_err_mean = np.mean(t_err)\n",
        "    t_err_median = np.median(t_err)\n",
        "    t_err_std = np.std(t_err)\n",
        "    t_err_min = np.min(t_err)\n",
        "    t_err_max = np.max(t_err)\n",
        "\n",
        "    print(\"RMS Error: \", t_err_rmse)\n",
        "    print(\"Mean Error: \", t_err_mean)\n",
        "    print(\"Median of Error: \", t_err_median)\n",
        "    print(\"Standard Deviation of Error: \", t_err_std)\n",
        "    print(\"Minimum Error: \", t_err_min)\n",
        "    print(\"Maximum Error: \", t_err_max)\n",
        "\n",
        "absolute_traj_error(t_gt, T_est)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Visual_Odometry.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}